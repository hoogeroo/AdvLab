{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental data analysis\n",
    "\n",
    "Please tell us who you are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aim\n",
    "\n",
    "The aim of this experiment is to illustrate the use of _Python_ for analysing and displaying experimental\n",
    "data. In particular, you will learn how to fit linear functions to measured data so as to extract meaningful\n",
    "experimental parameters, as well as their uncertainties. The techniques discussed in this experiment may be applied\n",
    "to many other Advanced Lab experiments and _you are encouraged to use __Python__ to analyse data obtained\n",
    "in other experiments_. In particular, this will greatly help you in quickly getting relevant uncertainties on your\n",
    "results.\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- The python documentation: http://docs.python.org/3/\n",
    "- Numerical python: http://www.numpy.org/\n",
    "- Scientific python: http://www.scipy.org/\n",
    "- Matplotlib: http://matplotlib.org\n",
    "- Code academy python: http://www.codecademy.com/tracks/python\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Python is a general-purpose interpreted language, which is high-level, has many extensions (known as\n",
    "modules) and is easy to learn. Furthermore, _Python_ is free (as in free beer). We use several of the extensions that have been added to _Python_ to make it more useful to Physicists. In the following code snippet, we preload _Numpy_ or numerical python, and _Matplotlib_, which is used for plotting.\n",
    "\n",
    "__import numpy as np__\n",
    "\n",
    "__import matplotlib.pyplot as plt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating means from experimental data\n",
    "\n",
    "In the simplest type of physical measurement, we repeatedly measure a quantity which we believe to be constant. Due to the\n",
    "inaccuracy of the measurement process, we do not always get the same result. Instead, we get a distribution of measured\n",
    "values about the true value, and we try to estimate the true value by various techniques such as by taking the average of the\n",
    "measurements. We need to know how good this estimate is likely to be.\n",
    "\n",
    "We usually assume that the distribution of measurements about the true value is a normal (Gaussian) distribution.\n",
    "Instead of actually making a set of measurements, we can let _Python_ generate a set of random data with a\n",
    "normal distribution by using the random number generator functions in _Python_. The *normal(mean, std,\n",
    "length)* function from the _np.random_ sub-module returns __length__ random samples from a normal\n",
    "distribution that has a mean of __mean__ and a standard deviation of __std__. As an example, we can use the following code:\n",
    "\n",
    "__y=np.random.normal(25, 3, 20)__\n",
    "\n",
    "__print(y)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the vector __y__ is a 20-element array. For a normal distribution, 68% of samples are\n",
    "expected to fall within one standard deviation of the mean. Record the number of elements of the vector __y__\n",
    "which lie between 22 and 28. What do you expect this number to be if the elements are normally distributed around 25 with a standard deviation of 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter your answer here.\n",
    "\n",
    "Now try:\n",
    "\n",
    "__plt.plot(y, 'o')__\n",
    "\n",
    "__plt.xlabel('Point #')__\n",
    "\n",
    "__plt.ylabel('Value')__\n",
    "\n",
    "__plt.show()__ \n",
    "\n",
    "\n",
    "As we may plot several lines at ones, we actively have to show the plot in order to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of the vector __y__ will be plotted as interconnected dots (that is the meaning of the __-o__ format specifier) as a function of the index.\n",
    "\n",
    "1. Plot out a graph of a 200 element vector __z__ (as a function of the index) whose elements are normally\n",
    "    distributed with mean 15 and standard deviation 2 using the following function <br>\n",
    "    __z=np.random.normal(15,2,200)__\n",
    "    <br> Plot your array __z__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now examine what happens as we take the average of the measurements. After making $n$ measurements, $\\left\\{\n",
    "z_{1},z_{2},\\ldots,z_{n}\\right\\}$, the mean $m$ of these measurements is\n",
    "$$\n",
    "m=\\dfrac{1}{n}\\left(  z_{1}+z_{2}+\\ldots+z_{n}\\right)\n",
    "$$\n",
    "It is interesting to see how $m$ changes as we make more and more measurements. Given the vector __z__ defined\n",
    "in (1) above, the following line calculates the mean of the first $1$, first 2, first 3, ... measurements and\n",
    "places the result in the array __zm=np.cumsum(z)__<br>\n",
    "Plot your resulting array __zm__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Enter the above and make sure you understand what it is doing. Explain this in your report. If you have not\n",
    "    met the cumulative sum function _np.cumsum_ before, try it out on a simple array such as\n",
    "    _np.array([1,2,3,4])_ and on a matrix array such as _np.array([[1,16],[3,9],[5,4],[7,1]])_.\n",
    "    _np.cumsum(a)_ will use the _flattened_ array (row-by-row), _np.cumsum(a,1)_ will sum along the\n",
    "    horizontal dimension (across rows) only.\n",
    "    \n",
    "Explain what _np.cumsum_ does here\n",
    "\n",
    "Note the use of the _np.arange(n)_ function above, which generates an array of subsequent integers\n",
    "starting from zero (from __0__ to __n-1__). Note that all _Python_ indexing are zero-based. The first element of the vector __z__ is __z[0]__ while the last one is __z[z.size-1]__.\n",
    "\n",
    "3. Plot out a graph of the successive means of the vector __z__ which you generated above and notice how\n",
    "    the mean approaches the true value as the number of points becomes large. Note, however, that there is always\n",
    "    some residual fluctuation which means that we can only _estimate_ the true value from measurements\n",
    "    --- there will always be some residual uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best straight line through a set of points\n",
    "\n",
    "In many applications, we have a set of $n$ data points $\\left(  x_{i}%\n",
    ",y_{i}\\right)  $ for which a linear relationship $y=ax+b$ is expected to hold between the variables. Due to\n",
    "measurement inaccuracies, the data points do not lie exactly on the line and we need to find the best values of $a$\n",
    "and $b$ given the data. The classical least-squares approach is to assume that the $x_{i}$ are known exactly. With\n",
    "this assumption, the aim is to find a straight\n",
    "line\n",
    "\n",
    "$$\n",
    "\\widehat{y}_{i}=ax_{i}+b\n",
    "$$\n",
    "\n",
    "that approximates to the $y_{i}$ such that the value of the sum of the squared\n",
    "deviations\n",
    "$$\n",
    "  \\varepsilon\\left(  a,b\\right) =\\left(  \\widehat{y}_{1}-y_{1}\\right)^{2} +\n",
    "                                 \\left(  \\widehat{y}_{2}-y_{2}\\right)^{2} + \\ldots +\n",
    "                                 \\left(  \\widehat{y}_{n}-y_{n}\\right)  ^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is straightforward to code this from scratch, we will use the _numpy_ routines to do the work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of polyfit\n",
    "np.polyfit, a part of the standard numpy/scipy library, fits polynomials of any degree to sets of data, returning the best fit parameters. Polyfit is invoked as:<br>\n",
    "__c = np.polyfit(x, y, d)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where __x__ is the x-axis data, __y__ is the y-axis data, and __d__ is the degree of the polynomial. The function returns __c__, a column vector of the best fit coefficients, starting with the coefficient of $x^d$. To try this out, we are going to lookup the height of the tides in Auckland harbour as a function of time, and fit a straight line to it. A straight line is a polynomial with degree 1.\n",
    "\n",
    "First we will import the data. For that, we use a library called _pandas_, which is good at dealing with comma-separated files (CSV). Plot the resulting __y__ vs __x__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "myurl='http://www.psmsl.org/data/obtaining/rlr.monthly.data/150.rlrdata'\n",
    "h=pandas.read_csv(myurl,sep=';').as_matrix()\n",
    "h=np.ma.masked_less(h,0) # There is a significant number of dead points in the data file. \n",
    "#We just need to filter those out. \n",
    "x=h[:,0] # the date is the first column\n",
    "y=h[:,1] # the water height is the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now use _np.ma.polyfit_ to create a fit through these points. _np.ma.polyfit_ works exactly the same as polyfit, but disregards the \"masked\" points. Usage is:<br>\n",
    "__c=np.ma.polyfit(x,y,order)__<br>\n",
    "For a fit to a straight line, the order is 1 (polynomial of order 1)<br>\n",
    "From the results, how many millimetres does the high tide mark increase per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "# Also make a plot of your data _and the fit_ on the same graph.\n",
    "# Use yp=np.polyval(p,x) where p is the fitted parameters, \n",
    "# and x is your x data to generate your fitted data\n",
    "\n",
    "# Remember to give us the increase per year!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted least squares fitting to a straight line\n",
    "\n",
    "In the previous sections, all of the data points were given equal weighting, i.e. all of the points were considered equally\n",
    "important when doing the fitting. In many cases we have a set of data where the standard errors in the points varies from\n",
    "point to point; in these situations we need to _weight_ the fitting so that the points which are known most accurately have a\n",
    "greater influence on the values of the fitted coefficients.\n",
    "\n",
    "The misfit function defined above must be changed to take this weighting into account. For a\n",
    "weighted least-squares problem, the misfit is called the chi-squared value and is defined as:\n",
    "$$\n",
    "  \\chi^{2}(c)  =\\sum_{i=1}^{n} \\dfrac{ \\left(  \\widehat{y}_{i}-y_{i}\\right)  ^{2}}{\\sigma_{i}^{2}}\n",
    "  \\label{eq:chisquare}\n",
    "$$\n",
    "where $\\sigma_{i}$ is the standard error in data point $i$, i.e. $y_i$ is known to within $\\pm \\sigma_i$. Again we\n",
    "assume that the $x_i$ are known perfectly. Note: Fitting a line through points with uncertainties in \\emph{both} the\n",
    "$x_i$ and the $y_i$ is much harder to tackle and will not be addressed here (see \\emph{Numerical Recipes} section\n",
    "15.3 if you are interested). In practice, if you encounter this case, you would arrange the fit such that the values\n",
    "with the largest uncertainties are chosen to be along the y-coordinate (and swap the $x_i$ and the $y_i$ otherwise).\n",
    "\n",
    "Polyfit can be made to use _weighted_ data points using the following syntax: <br>\n",
    "__c=np.polyfit(x,y,1,w=1/sigma)__ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where __sigma__ is an array of the same length as __x__ and __y__ that contains the uncertainties in the points. \n",
    "\n",
    "5. Below we will generate some mock data, with uncertainties. Add your code to obtain the best value for a straight line fit to this data, including the uncertainties. Make a plot of your mock data with the straight line fit on the same graph. \n",
    "\n",
    "In order to see the errorbars on the points, you should also use the function <br>\n",
    "__plt.errorbar(x,y,sigma)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 5, 0.5)\n",
    "y = np.array([2.75, 4.83, 5.05, 6.80, 8.83, 8.64, 11.03, 13.20, 13.08, 15.68]) \n",
    "sy = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0, 1.1, 1.1, 1.2])\n",
    "# Add your code here to do the fit and plot the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainties in the parameters\n",
    "\n",
    "Because the data points $y_{i}$ are not known perfectly (only to within $\\pm\\sigma_i$), it should be clear that we\n",
    "cannot determine perfectly the fitting coefficients $a$ and $b$ of the least-square linear fit. There is a certain\n",
    "probability that $a$ and $b$ will deviate from the nominal values returned by \\texttt{wregress}. In other words, $a$\n",
    "and $b$ have their own uncertainties, $a \\pm \\sigma_a$ and $b \\pm \\sigma_b$. It is important to estimate those\n",
    "uncertainties: In many experiments we extract the parameter we are after through a linear fit, i.e the value of $a$\n",
    "(or $b$) is the ultimate result of the experiment, and we want to know how accurately it has been determined.\n",
    "\n",
    "As _all_ of the data points $y_{i}$ are used in the determination of the fit parameters $a$ and $b$, they each\n",
    "contribute some of their own uncertainty to the uncertainty of the fit parameters. In order to estimate the error in\n",
    "the fit parameters we can use the standard _error propagation_ formula. The total error~$\\sigma_a$ in parameter~$a$\n",
    "is the quadrature-sum (square root of the sum of the squares) of the contributions of the errors in each data point\n",
    "to the error in $a$. The contribution is given by the general formula which is the basis for all error calculations:\n",
    "$$\n",
    "  \\sigma_{a}^{2}=\\sum_{i=1}^{n}\\left[  \\sigma_{i}^{2}\\left(  \\dfrac{\\partial a}{\\partial y_{i}}\\right)  ^{2}\\right]\n",
    "$$\n",
    "\n",
    "The good news is that this can all be done by the _polyfit_. The covariance matrix can be returned by polyfit if we use the following syntax:<br>\n",
    "__c,cv=np.polyfit(x,y,1,w=1/sy,cov=True)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances (the squares of the uncertainties) are returned as the diagonal elements of the covariance matrix __cv__ .\n",
    "6. Use the code blocks above and below to give the uncertainties in the fitted parameters. Make sure to print out the fitted parameters as well as their uncertainties. The diagonal elements from a matrix __cv__ can be found using <br>\n",
    "__np.diag(cv)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Please tell us what you have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
